# P-GAT : Pose-Graph Attentional Network for Lidar Place Recognition

This repository contains the code implementation for the paper "P-GAT : Pose-Graph Attentional Network for Lidar Place Recognition" accepted for publication in IEEE Robotics and Automation Letters (RA-L, 2024).

Utilising an attentional graph neural network, P-GAT compares (key)nodes between sequential and non-sequential sub-graphs for place recognition tasks as opposed to a common frame-to-frame retrieval problem formulation currently implemented in SOTA place recognition methods.

```@ARTICLE{ramezani2023pose,
  author={Ramezani, Milad and Wang, Liang and Knights, Joshua and Li, Zhibin and Pounds, Pauline and Moghadam, Peyman},
  journal={IEEE Robotics and Automation Letters}, 
  title={Pose-Graph Attentional Graph Neural Network for Lidar Place Recognition}, 
  year={2024},
  volume={9},
  number={2},
  pages={1182-1189},
  doi={10.1109/LRA.2023.3341766}}
  ```

The arXiv version of P-GAT (Extended version of the RA-L paper) can be found here ([link](https://arxiv.org/abs/2309.00168)).

## Requirements
The code was tested using Python 3.9.4 with PyTorch 1.11.0. To set up the complete environment, we recommend using the following commands:
```
conda create -n `<myenv>` python=3.9.4
conda activate `<myenv>`
```
Replace the `<myenv>` by any name you want.
```
pip install -r requirements.txt
conda activate <myenv>
```

## Datasets
For the training and evaluation, P-GAT uses three datasets: 
- Oxford RobotCar dataset
- In-house dataset
- MulRan dataset

We follow MinkLoc3D ([link](https://github.com/jac99/MinkLoc3D)) to generate the descriptors for point clouds and save them in pickles. Oxford dataset can be downloaded from the PoinNetVLAD repository ([link](https://github.com/mikacuy/pointnetvlad)). MulRan dataset can be downloaded from ([link](https://sites.google.com/view/mulran-pr/dataset)). As an example, pickles generated for Oxford (based on MinkLoc3D) can be downloaded from ([link](https://www.dropbox.com/home/P-GAT/oxford_pickles)) (Note: Dataset preparation for Oxford may require up to one day. For conveninece, the elements needed for evaluation on Oxford can be downloaded here ([link](https://www.dropbox.com/home/P-GAT/oxford_eval)))

We use `datasets/dataset_generator_oxford.py` to process the Oxford and in-house dataset for training and testing, `datasets/dataset_generator_mulran.py` to process DCC and riverside sub-datasets in MulRan dataset, and `datasets/dataset_generator_kaist.py` to process KAIST sub-dataset in MulRan. 

A global graph and sub-graphs are generated inside the scripts. Note: The global graph and its corresponding adjacency matrix are exclusively used for training purposes.
The usage of the scripts is:
```
usage: dataset_generator.py [-h] [--config_file FILE] ...

Generate dataset in graph structure

positional arguments:
  opts                Modify config options using the command-line

optional arguments:
  -h, --help          show this help message and exit
  --config_file FILE  path to config file
```
The arguments required by the script is maintained by the YAML configuration files. The example of the config file is `configs/data_generator_config.yml`, and an example of usage command is:
```
python dataset_generator_oxford.py --config <path-to-config>/data_generator_config.yml
```

## Model training
We use `training/train.py` to train the model. 

The default configuration is in the `attentional_graph/config/defaults.py`.
The customised configuration is applied via configuration files in format of YAML. 
The example of config file in YAML can be found in `configs/training_config.yml`.

The example of training command is:
```
python training/train.py --config <path-to-config>/training_config.yml
```

## Backbone Models
Checkpoints listed below are the P-GAT models trained based on the descriptors generated by PointNetVLAD, MinkLoc3D and PPTNet on the Oxford and MulRan datasets.

| Model      | Checkpoint | Pre-trained on |
| ----------- | ----------- | ----------- |
| PointNetVLAD      | Checkpoint([link](https://www.dropbox.com/home/P-GAT/models?preview=pgat_pointnetvlad_oxford.pt))       | Oxford |
| MinkLoc3D   | Checkpoint ([link](https://www.dropbox.com/home/P-GAT/models?preview=pgat_minkloc3d_oxford.pt))       | Oxford |
| PPT-Net   | Checkpoint([link](https://www.dropbox.com/home/P-GAT/models?preview=pgat_pptnet_oxford.pt))        | Oxford |
| PointNetVLAD   | Checkpoint([link](https://www.dropbox.com/home/P-GAT/models?preview=pgat_pointnetvlad_mulran.pt))        | MulRan |
| MinkLoc3D   | Checkpoint([link](https://www.dropbox.com/home/P-GAT/models?preview=pgat_minkloc3d_mulran.pt))        | MulRan |
| PPTNet   | Checkpoint([link](https://www.dropbox.com/home/P-GAT/models?preview=pgat_pptnet_mulran.pt))        | MulRan |


## Model evaluation
We use `training/test.py` to evaluate the model.

Same as the training, the default configuration is in the `attentional_graph/config/defaults.py`,
and the customised configuration is applied via configuration files in format of YAML. 
The example of config file in YAML can be found in `configs/test_config.yml`.

The example of testing command is:
```
cp <pre-trained-model.pt> .
python training/test.py --config <path-to-config>/test_config.yml
```
## Acknowledgement
P-GAT has been developed based on the publically available lidar place recongition approaches including MinkLoc3D ([link](https://github.com/jac99/MinkLoc3D)), PointNet-VLAD ([link](https://github.com/mikacuy/pointnetvlad)) and PPT-Net([link](https://github.com/fpthink/PPT-Net)).

P-GAT is inspired by the attentional graph neural network developed in SuperGlue ([link](https://github.com/magicleap/SuperGluePretrainedNetwork)).